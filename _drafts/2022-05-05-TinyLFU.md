---
layout: post
title: "[論文解讀][TinyLFU] A Highly Efficient Cache Admission Policy"
description: ""
category: 
- 論文解讀
tags: ["bloom filter"]

---

![image-20220504162659830](../images/2021/image-20220504162659830.png)

# 以前的問題 

![What is Memory Hierarchy: Definition, Diagram, Architecture and Advantages](https://www.elprocus.com/wp-content/uploads/Memory-Hierarchy.jpg)

不論是記憶體或是任何資料儲存體（或是大家常用的 Redis），都會使用 Cache 來存放短期會反覆使用到的資料。 因為 cache 具有讀取快速，但是容量相對小（且價格較貴）。所以無法將在 Physical memory 的資料全部存上，這時候就是需要有 Cache Replacement Policy (也就是「快取的取代策略」)

一般來說的做法有以下幾種：

- **FIFO (First-In First-Out):** 透過先進先出的方式，往往這樣容易有相當大的功耗在於資料不斷的換進來與換出去。
- **LRU(Least Recently Used)**: 取代資料的時候，會去選擇近來最久的。   (RecentlyUsed --)
- **LFU(Least Frequency Used):** 取代資料的時候，選擇最少被讀取到的。 (Read Count ++)

那麼這些聽起來好像都沒有問題，為什麼會需要有 **TinyLFU** ?

## LFU 有什麼問題？

- **操作 LFU 時間複查度是**： $$O(log(n))$$
- 由於取代方案取代方式是挑選短期（取決 cache 大小），來挑選最長使用的。 但是可能發生一個狀況是，如果某份資料短期不常使用，但是長期來說他頻率確是最高的。反而這個狀況會造成沒有效率。

那要如何解決這些問題呢？

# 什麼是 TinyLFU 是做什麼用？

![image-20220504173929676](../images/2021/image-20220504173929676.png)

# 






## Sample Code

https://github.com/dgryski/go-tinylfu



## Paper: [https://arxiv.org/abs/1512.00727](https://arxiv.org/abs/1512.00727)

```
TinyLFU: A Highly Efficient Cache Admission Policy

Gil Einziger, Roy Friedman, Ben Manes

This paper proposes to use a frequency based cache admission policy in order to boost the effectiveness of caches subject to skewed access distributions. Given a newly accessed item and an eviction candidate from the cache, our scheme decides, based on the recent access history, whether it is worth admitting the new item into the cache at the expense of the eviction candidate.
Realizing this concept is enabled through a novel approximate LFU structure called TinyLFU, which maintains an approximate representation of the access frequency of a large sample of recently accessed items. TinyLFU is very compact and light-weight as it builds upon Bloom filter theory.
We study the properties of TinyLFU through simulations of both synthetic workloads as well as multiple real traces from several sources. These simulations demonstrate the performance boost obtained by enhancing various replacement policies with the TinyLFU eviction policy. Also, a new combined replacement and eviction policy scheme nicknamed W-TinyLFU is presented. W-TinyLFU is demonstrated to obtain equal or better hit-ratios than other state of the art replacement policies on these traces. It is the only scheme to obtain such good results on all traces.
```



# Redis 有哪一些 Cache Replacement Policy ?

參考： [93面試常問：Redis 記憶體滿了怎麼辦？](https://iter01.com/557774.html)

實際上Redis定義了幾種策略用來處理這種情況：

- **noeviction(預設策略)**：對於寫請求不再提供服務，直接返回錯誤（DEL請求和部分特殊請求除外）
- **allkeys-lru**：從所有key中使用LRU演算法進行淘汰
- **volatile-lru**：從設定了過期時間的key中使用LRU演算法進行淘汰
- **allkeys-random**：從所有key中隨機淘汰資料
- **volatile-random**：從設定了過期時間的key中隨機淘汰
- **volatile-ttl**：在設定了過期時間的key中，根據key的過期時間進行淘汰，越早過期的越優先被淘汰






# Reference

- [LRU and LFU Cache Algorithms](https://xuri.me/2016/08/13/lru-and-lfu-cache-algorithms.html)

- [W-TinyLFU论文阅读与原理分析​](https://www.modb.pro/db/218970)

- [阅读 redis 源码，学习缓存淘汰算法 W-TinyLFU](https://mytechshares.com/2021/11/07/redis-known-lru-wtinylfu/)

- [[论文《TinyLFU: A Highly Ecient Cache Admission Policy》阅读笔记](https://segmentfault.com/a/1190000016091569)](https://segmentfault.com/a/1190000016091569)

-  [93面試常問：Redis 記憶體滿了怎麼辦？](https://iter01.com/557774.html)

- [dgryski](https://github.com/dgryski)/**[go-tinylfu](https://github.com/dgryski/go-tinylfu)**
